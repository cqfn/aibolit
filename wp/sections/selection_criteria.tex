There is a number of crucial expectations we have for the
method and the tool under design:

\textbf{Novelty} is crucial and has a few important aspects:
\begin{enumerate*}
\item 
Novelty of the \emph{method} is one the most important values of novelty. 
E.g.,~\citet{Akiyama1971AnEO} was the first who 
tried to solve defect detection problem using 
linear regression as it was mentioned by~\citet{7476771}.
Sometimes authors can use some new idea in the 
algorithm, e.g,~\citet{XIAO201917} used convolutional neural network
together with word-embedding and feature-detecting techniques.

\item
The \emph{effectiveness} of the model is another important 
value of novelty. If authors published new model which 
is more effective, it means that we can solve a problem 
much better. All evaluation measures can be described later.

\item
Collecting new dataset is one of the most important and hard work in Data Science. 
Main results can vary depending on method used when gathering a dataset. 
It is necessary to do it in a correct way, since there are many issues related 
to data problems like \emph{Outliers}, \emph{Class Imbalance}, \emph{Data shift Problem}, 
\emph{High Dimensionality of Data}, etc. mentioned by~\citet{10.1007/s10462-017-9563-5}.
Also, \emph{granularity} is another problem in the 
defect detection problem. Defect can be found in a module, 
function, line of the code, etc. That is why seems 
it is not an easy task to compare the models with a
different level of granularity.
E.g,~\citet{6464273} analyzed five papers published by 
IEEE and mentioned that there are some data 
quality issues in these papers.
\end{enumerate*}

\textbf{Evaluation} is the essential of ML. It is impossible 
to say whether a model is good or bad, not evaluating it. 
A lot of model performance evaluation measures were used 
for defects detection, but we can 
classify them the following way \citep{10.1007/s10462-017-9563-5, Jiang2008}:
\begin{enumerate*}[label=\arabic*)]
\item \emph{Numeric} performance evaluation measures
are mostly used for defect detection models~\citep{Jiang2008}. 
They can include accuracy, F1-score, G-means, specificity, 
f-measure, and so on. E.g., they are used by~\citet{6349519}. 
\item \emph{Graphical} measures are graphs derived from 
the confusion matrix~\citep{Jiang2008}. 
They can include ROC curve, precision-recall curve, cost curve etc. 
E.g., they are used by~\citet{SHATNAWI20081868}.
\end{enumerate*}

\textbf{Performance} is another important value for defect detection model. 
The selection of the best performance evaluation measure 
is not a trivial task.
For example,~\citet{Jiang2008} compared different alternatives
and demonstrated that no single performance evaluation measure is able to evaluate the
performance of a defect prediction model fair enough. The authors also added 
that it is better not only to measure model classification performance 
(like accuracy, recall, etc.), but to minimize the misclassification cost.
\citet{ARISHOLM20102} also compared different types of measures 
and mentioned that it is hard to draw general conclusions about the 
best performance evaluation measure.

Since the tool will be actively used with Continuous Integration (CI), 
it must have a reasonable performance: it must predict in a few seconds. 
If calculation takes a lot of time, a developer may stop using it.
\citet{humble2010continuous} also mentioned that speed 
is very important for CI. The sooner you release the software, 
the sooner you get a return on your investment. 
Performance is also important because it is necessary 
to know whether bugfixes are useful. That is why
we need to minimize the delay between the releases and 
thus, accelerate the feedback, as explained by~\citet{humble2010continuous}.

State-of-the-art papers use Neural Networks (NN) for defect 
detection problem since it can give better results~\citep{XIAO201917,10.1145/3360588}. 
Training takes a lot of time and demands TPU or GPU resources if we use NN. 
Word embedding and other feature-detecting techniques can only increase training time. 
E.g., \citet{10.1145/3360588, 8616596} noticed that 
the training time matters for defect detection problem and 
demonstrates the training time of their models.

